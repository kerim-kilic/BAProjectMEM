---
title: |
  Business Analytics: Damm Company Analysis
author: "Kerim Kiliç, Georgi Gevorgyan, Mar Muñoz, Alejandro Urrea"
date: '`r Sys.Date()`'
output: 
    prettydoc::html_pretty:
      toc: true
    theme: hpstr
    highlight: github
---


```{r setup, include = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE,message = FALSE,warning = FALSE)
library(tidyverse)
library(gridExtra)
library(janitor)
library (lubridate)
library(arules)
library(formattable)
library(rfm)
library("CLVTools")
library(mapSpain)
library(arulesViz)
Sys.setenv(LANG = "sp")
```

# Introduction

This documents aims to  help the company DDI to develop a good commercial strategy based on data, making specific marketing proposals for the different products. In order to do so, first the data will be cleaned. Following, a general overview of the data available will be shown. Then, a RFM analysis will take place, followed by a CLV one. After this, some association rules will be studied. Lastly, this project's conclusions will be presented. 

# Software Setup

The software used for the development of the study and the writing of the report is R[1]. The first step is to define the work directory and to load the libraries: 

* library(tidyverse) [2]
* library(gridExtra) [3]
* library(janitor) [4]
* library (lubridate) [5]]
* library(arules) [6]
* library(formattable) [7]
* library(rfm) [8]
* library("CLVTools") [9]
* library(mapSpain) [10]
* library(arulesViz) [11]

# Data

The data used in this report is the one provided by DDI. This data is located in 4 different tables, which are: 

* t_detallista
* material_preu_damm
* t_material_Freqs
* t_posiciones


Now, the data will be retreived from the four tables just mentioned. 

```{r, echo = FALSE, message = FALSE}
retailers <- read.csv("data/t_detallista.csv", encoding = 'UTF-8')
prices <- read.csv("data/material_preu_damm.csv", encoding = 'UTF-8')
quantity <- read.csv("data/t_material_Freqs.csv", encoding = 'UTF-8')
orders <- read.csv("data/t_posiciones.csv", encoding = 'UTF-8')
```

[Describe each table and their corresponding variables.]

The table t_datallista, from now on referred as retailers. This data refers to XXX. 

In addition, the table material_preu_damm, now referred as prices. This data includes XXX. 

Then, there is the table t_material_Freqs, now called quantity. This data refers to XXX.  

Lastly, there is the table t_posiciones, now called orders. This data includes information about the orders placed by each establishment. It includes the ID of the establishment, the date the order was placed, as well as which material was ordered, in which unit of measure and its quantity. 

The data had some variable names in Spanish. Now, these are translated to English for a better understanding.

```{r}
colnames(retailers) <- c("X", "id_retailer", "id_establishment", "country", 
                         "distributor", "region", "area", "category","type", 
                         "type2", "customer")
colnames(prices) <- c("material","price","volume_minor_unit","quantity_minor_units",
                      "volume_unit", "price_liter")
colnames(orders) <- c("X", "id_establishment", "date", "material", "customer", 
                      "measure_unit", "quantity")
```

# Data cleaning

In this section, the original data provided will be cleaned. In order to do that, first a function to clean the data will be defined. This function will clean column titles, remove empty rows and columns, remove constant variables, and remove duplicate values. 


```{r}
clean_data <- function(dataset)
{
  ### Clean the names of the variables ###
  dataset <- dataset %>% clean_names()
  ### Remove the empty rows and columns ###
  dataset <- dataset %>% remove_empty(which = c("rows","cols"))
  ### Remove columns with constants ###
  dataset <- dataset %>% remove_constant()
  ### Remove duplicate values ###
  dataset <- dataset %>% distinct()
  return(dataset)
}
```

## Cleaning Data "Orders"

Now, the table Orders will be cleaned. 

First,  the clean_data() function is run to clean the names of the variables, remove the empty rows and columns, remove the columns with only a constant value, remove the duplicate values, and remove the column x since it does not give any important information.


```{r}
orders <- clean_data(orders) %>% select(-x)
```

The "id_establiment" variable  
```{r}
orders %>% tabyl(id_establishment) %>% select (id_establishment) %>% summary()
```
There are NA's values, which must be removed. The other values seem to be correct. 
```{r}
orders <- orders %>% filter(!is.na(id_establishment))
```

The "Date" variable
```{r}
orders <- orders %>% mutate(date =as.Date(date, format = "%d.%m.%Y"))
```

Looking at the first dates and last dates
```{r}
orders$date %>% head(20)
orders$date %>% tail(20)
```

It is assumed that 0202 should be 2002, that 2218 is 2018, that 2202 is 2022 and that 2119 is 2019. 

```{r}
orders$date <- ifelse(orders$date == "0202-03-01", "2002-03-01", as.character(orders$date))
orders$date <- ifelse(orders$date == "2218-04-09", "2018-04-09", as.character(orders$date))
orders$date <- ifelse(orders$date == "2119-12-05", "2019-12-05", as.character(orders$date))
orders$date <- ifelse(orders$date == "2202-12-20", "2020-12-20", as.character(orders$date))
orders$date <- ifelse(orders$date == "2024-01-01", "2021-01-01", as.character(orders$date))
orders$date <- ifelse(orders$date == "2022-07-14", "2021-07-14", as.character(orders$date))
orders <- orders %>% mutate(date =as.Date(date, format = "%Y-%m-%d"))
```

The "Customer" variable

```{r}
orders %>% tabyl(customer) %>% select (customer) %>% summary()
```

The NA's are deleted. The other values seem to be correct. 
```{r}
orders <- orders %>% filter (!is.na(customer))
```


The "Unit" variable
```{r}
orders %>% tabyl(measure_unit)
empty_unit <- nrow(orders %>% filter(is.na(measure_unit)))
empty_unit
```

There are `r empty_unit` with no unit of measure

The "quantity" variable
```{r}
orders %>% tabyl(quantity) %>% select (quantity) %>% summary()
```

```{r}
zero_quantity <- nrow(orders[orders$quantity == 0,])
```

There are `r zero_quantity` with zero quantity

Those rows with a 0 in quantity are deleted. 
```{r}
orders <- orders %>% filter(quantity != 0)
```

## Cleaning Data "Quantity"
In this section, the table quantity will be cleaned. First,  the clean_data() function will be run to clean the names of the variables, remove the empty rows and columns, remove the columns with only a constant value, remove the duplicate values, and remove the column x since it does not give any important information.

```{r}
quantity <- clean_data(quantity) %>% select(-x)
which(colSums(is.na(quantity)) > 0)
```

The NA's values of the material column are removed. 
```{r}
quantity <- quantity%>%filter(!is.na(material))
```


### Adding the type of product

It was seen that the products could be grouped by product type. This agrupation is shown now. 
The product types are: 

* AK
* BOCK
* COMPLOT
* LEMON
* ESTRELLA
* FREE
* INEDIT
* SAAZ
* TURIA
* VOLL
* WEISS
* XIBECA
* GENERAL
* GRUPO DAMM
* DAURA
* DAMM
* EQUILATER

```{r}
prices <- prices %>%
  mutate(type = case_when(grepl("AK", material,  fixed = TRUE) ~ "AK",
                          grepl("BOCK", material,  fixed = TRUE) ~ "BOCK",
                          grepl("COMPLOT", material,  fixed = TRUE) ~ "COMPLOT",
                          grepl("LEMON", material,  fixed = TRUE) ~ "LEMON",
                          grepl("ESTRELLA", material,  fixed = TRUE) ~ "ESTRELLA",
                          grepl("FREE", material,  fixed = TRUE) ~ "FREE",
                          grepl("INEDIT", material,  fixed = TRUE) ~ "INEDIT",
                          grepl("SAAZ", material,  fixed = TRUE) ~ "SAAZ",
                          grepl("TURIA", material,  fixed = TRUE) ~ "TURIA",
                          grepl("VOLL", material,  fixed = TRUE) ~ "VOLL",
                          grepl("WEISS", material,  fixed = TRUE) ~ "WEISS",
                          grepl("XIBECA", material,  fixed = TRUE) ~ "XIBECA",
                          grepl("GENERAL", material,  fixed = TRUE) ~ "GENERAL",
                          grepl("GRUPO DAMM", material,  fixed = TRUE) ~ "GRUPO DAMM",
                          grepl("DAURA", material,  fixed = TRUE) ~ "DAURA",
                          grepl("DAMM", material,  fixed = TRUE) ~ "DAMM",
                          grepl("EQUILATER", material,  fixed = TRUE) ~ "EQUILATER"))
```

## Merging orders and prices

In order to perform the analysis, the tables orders and prices should be merged. This is done using the merge function, and doing it by the "material" column In order to have relevant data, this data is filter from 2002 on wards. 

```{r}
# Merge orders and prices
orders_prices <- merge(orders,prices,by="material")
orders_prices <- filter(orders_prices, date > "2002-01-01")
```


## Cleaning Data "Retailers"

In this section, the table retailers will be cleaned. First,  the clean_data() function is run to clean the names of the variables, remove the empty rows and columns, remove the columns with only a constant value, remove the duplicate values, and remove the column x since it does not give any important information.

```{r}
retailers <- clean_data(retailers) %>% select(-x)
```

Now, it is checked which columns/variables have missing data
```{r}
which(colSums(is.na(retailers)) > 0)
```

Looking at the types of the restaurants
```{r}
nrow(retailers[is.na(retailers$type),])
nrow(retailers[is.na(retailers$type2),])
```

It can be seen that there are `r nrow(retailers[is.na(retailers$type),])` entries out of `r nrow(retailers)` total that are missing the type of the restaurant, and only `r nrow(retailers[is.na(retailers$type2),])` that are missing 'type2'. Thus,  "type2" will be treated as the general type of the restaurant, and in case missing, it will be replaced with "type" and in case both are missing, it will be considered deleting that entry.

```{r echo=FALSE, include = FALSE}
ifelse(!is.na(retailers$type2), retailers$type <- retailers$type2, retailers$type )
retailers$type2 <- NULL
```

Now, the other missing variables are checked. 
```{r}
which(colSums(is.na(retailers)) > 0)
retailers[is.na(retailers$country),]
```

For country, the last two missing entries are clearly in Spain, the first one has already other missing variables as well, so it will be delete it.

```{r}
retailers[retailers$id_retailer == '9100245153',]$country <- 'España'
retailers[retailers$id_retailer == '9100366860',]$country <- 'España'
retailers <- subset(retailers, retailers$id_retailer != '9100399677')
```

Now, the area and region are studied. 
```{r}
nrow(retailers[is.na(retailers$area),])
nrow(retailers[is.na(retailers$region),])
retailers[is.na(retailers$region),]
```

It is seen that there are only `r nrow(retailers[is.na(retailers$region),])` missing region variable and `r nrow(retailers[is.na(retailers$area),])` area entries. Furthermore, within the scope of this analysis it won't be looked through the specific areas within the cities or regions, and so the *area* variable is redundant and can be removed at all.

```{r}
retailers$area <- NULL
retailers <- subset(retailers, !is.na(retailers$region))
tabyl(retailers$region)
```

Looking through the category data a lot of NA values can be seen, and after checking with the company, it was realized that they are not classified yet. Thus, they will be marked as so for now and classify them afterwards as part of the work. 

```{r}
tabyl(retailers$category)
retailers$category <- ifelse(is.na(retailers$category), "Non_classified", retailers$category)
```


Last up is the customer values.

There are only `r nrow(retailers[is.na(retailers$customer),])` customer values missing. Since it might be needed clean customer values for later merging with orders table, it will just be removed the rows with NAs, as it will not affect the data anyhow.

```{r}
retailers <- subset(retailers, !is.na(retailers$customer))
```

## Cleaning '/ES' values

There are many retailers for which the field **Region** has a value of '/ES'. However, further inspection has shown that in many cases the same establishment has a determined region in another entry. This fact was used to replace the undetermined values '/ES' by a valid value in other entries.

This was done by splitting the data frame in one part containing the values '/ES' in region and another with the rest. Then, the known regions were combined into the unknown by using 'id_establishment'. Finally, both data frames were combined by using **rbind** function.

```{r}
retailers_ES <- retailers %>% filter(region == "ES/")
retailers_noES <- retailers %>% filter(region != "ES/")
retailers_noES_unique <- retailers_noES %>% distinct(id_establishment, region)

retailers_ES <- merge(retailers_ES, retailers_noES_unique, by = "id_establishment")
retailers_ES <- retailers_ES %>% select(-region.x) %>%
mutate(region = region.y) %>% select(-region.y)

retailers <- rbind(retailers_noES, retailers_ES)
```

The method previously described worked for all the entries in which the value was '/ES'. Additionally, there were a few entries with value '/'. These were simply deleted from the data set.

```{r}
retailers <- retailers %>% filter(region != "/")
```


# General overview of the data 

In this section of the report, a general overview of the data will be shown using graphs. First, the evolution of order lines over years and month. Then, the top clients will be observed. Following, the most popular client locations will be analysed, as well as the preferred product type. 

## Order lines per product type
As some orders include several order lines, and some only one, this analysis will be done regarding order lines. 

First, the number of times a specific article type appears in the dataset is counted, having like that the number of order lines referring to this type of product, Then, from this, a graph taking into account the percentage from the total number of order lines is made.  

```{r, fig.align='center'}
orders_prices %>%
  group_by(type)%>%
  summarise(count = n()) %>%
  mutate(percentage =  round((count/nrow(orders_prices))*100,2)) %>%
  ggplot(aes(reorder(type, percentage), percentage, fill=type))+
  geom_text(aes(label=percentage), hjust=-0.01)+
  geom_bar(stat="identity", position = "dodge") + 
  coord_flip() + 
  labs(title= "% of Orders per Product Type", y = "% of Order lines", x = "Type of prodcut") +  
  theme(legend.position = "none")
```

It can be seen that the most popular product type is Estrella, followed by Free. In the last positions there are Xibeca and Equilated. 

## Revenue per product type
The same analysis done in the previous point will now be presented taking into account the revenue instead of the number of order lines. 

```{r, fig.align='center'}
orders_prices_revenue <- orders_prices %>%
  mutate(Revenue = price_liter * quantity)

total_rev =  sum(orders_prices_revenue$Revenue)

orders_prices_revenue %>%
  group_by(type)%>%
  summarise(rev = sum(Revenue)) %>%
  mutate(percentage_rev =  round((rev/total_rev)*100,2)) %>%
  ggplot(aes(reorder(type, percentage_rev), percentage_rev, fill=type))+
  geom_text(aes(label=percentage_rev), hjust=-0.01)+
  geom_bar(stat="identity", position = "dodge") + 
  coord_flip() + 
  labs(title= "% of Revenue per Product Type", y = "% of Revenue", x = "Type of prodcut") +  
  theme(legend.position = "none")

```

It can be seen that this graph is aligned with the previous one. It shows that by far, the product which more revenues represents is Estrella, with a 66%. Then there is Turia, followed by Free and Lemon. In the lower part of the scale, Xibeca and Equilater are found. 

## Order lines over the years 

Now, an evolution of the order lines over the years will be shown. The data is filtered from 2015 to 2022 in order to have relevant information. 

```{r, fig.align='center'}
orders$Year <- as.Date(cut(orders$date,
                            breaks = "year"))

orders %>%  filter(date >= "2015-01-01" & date <= "2022-01-01") %>% group_by(Year) %>% 
  summarise(count = n()) %>% ggplot(aes(Year, count)) + geom_line() +
  geom_point()  + 
  labs(title= "Order Lines over Years", y = " Order lines", x = "Year ") + theme_minimal()
```
## Revenue over the years 

```{r, fig.align='center'}
orders_prices_revenue$Year <- as.Date(cut(orders_prices_revenue$date,
                            breaks = "year"))

orders_prices_revenue %>%  filter(date >= "2015-01-01" & date <= "2022-01-01") %>% group_by(Year) %>% 
  summarise(rev = sum(Revenue)) %>% ggplot(aes(Year, rev)) + geom_line() +
  geom_point()  + 
  labs(title= "Revenue over Years", y = " Order lines", x = "Year ") + theme_minimal()
```

It can be seen that the revenue graph does not follow the same pattern as the number of order lines one. This is due to the fact that one order line can represent a lot of revenue if the quantity of that order line is really big, or the price of the prodcut is high. It should be noted the peak in revenue in 2018, not seen in the number of order lines graph. 

## Order lines over the months   
Now, an evolution of the order lines over the months in several years will be shown. The data is filtered from January 2018 to Mat 2021. 

```{r, fig.align='center'}
orders$Month <- as.Date(cut(orders$date,
                            breaks = "month"))


a1<- orders %>% filter(date >= "2018-01-01" & date <= "2021-05-30") %>% group_by(Month) %>% 
  summarise(count = n())

  
a1%>%ggplot(aes(Month, count)) + geom_line() +
  geom_point()+
  geom_label(data = a1 %>% filter(month(Month) == 1), aes(label=count))+
  geom_vline(xintercept = as.numeric(as.Date("2020-03-15")), linetype=5, colour="red")+
  labs(title= "Order Lines over Months", y = " Order lines", x = "Month ") + theme_minimal()
```
It can be seen that the peaks are located in the month of January of every year The red line shows the moment COVID-19 pandemic restrictions started in Spain. A clear decline in order lines can be seen after this. The peak in 2021 is much lower than the previous ones. 

## Revenue over the months   

```{r, fig.align='center'}

orders_prices_revenue$Month <- as.Date(cut(orders_prices_revenue$date,
                            breaks = "month"))


a2<- orders_prices_revenue %>% filter(date >= "2018-01-01" & date <= "2021-05-30") %>% group_by(Month) %>% 
  summarise(rev = round(sum(Revenue),0))

  
a2%>%ggplot(aes(Month, rev)) + geom_line() +
  geom_point()+
  geom_label(data = a2 %>% filter(month(Month) == 1), aes(label=rev))+
  geom_vline(xintercept = as.numeric(as.Date("2020-03-15")), linetype=5, colour="red")+
  labs(title= "Revenue over Months", y = " Order lines", x = "Month ") + theme_minimal()
```
Here again, it can be seen that the revenue peaks are in January. Again, the 2018 revenue peak mention before, it is seen again. After COVID, the revenue, as the order lines did, decreased. 

## Retailers order lines per location

It is interesting to study where the most order lines come from in terms of location. This will be analysed now. 

```{r, fig.align='center'}
tab_3 <- retailers %>%
  group_by(region)%>%
  summarise(count = n()) %>%
  mutate(percentage =  round((count/nrow(retailers))*100,2))
  
tab_4 <- tab_3[order(-tab_3$percentage),] 

tab_4%>%head(15)%>%
  ggplot(aes(reorder(region, percentage), percentage, fill=region))+
  geom_text(aes(label=percentage), hjust=-0.01)+
  geom_bar(stat="identity", position = "dodge") + 
  coord_flip() + 
  labs(title= "% of retailers per region", y = "% of Retailers", x = "Region") +  
  theme(legend.position = "none")

```
It can be seen that more than 30% of the retailers are located Barcelona, whereas less than 2% are in Vizcaya. 

## Geographical analysis
It was also considered of interest the geographical distribution of variables such as\ sales, expressed in terms of money and in volume.

To perform this analysis, an external data set 'Provinces' with geographical and population information was imported. By using merge in several occasions, this kind of data was connected with the orders.

```{r}
#retrieving info about communities and provinces
provinces <- read.csv("data/capitals-locations.csv", fileEncoding = 'UTF-8-BOM')
autonomous_com <- read.csv("data/community_population.csv")

#necessary to modify the actual coordinates of the Canary island, because in our
#they are depicted closer to the peninsula. 
provinces$Province <- str_to_title(provinces$Province)
retailers$region <- str_to_title(retailers$region)

#necessary to modify the actual coordinates of the Canary island, because in our map
#they are depicted closer to the peninsula.
provinces$Latitude <- ifelse(provinces$Province == "Las Palmas", 35.1, provinces$Latitude)
provinces$Longitude <- ifelse(provinces$Province == "Las Palmas", -10.5, provinces$Longitude)

provinces$Latitude <- ifelse(provinces$Province == "Sta Cruz De Tenerife",
                            35.5, provinces$Latitude)
provinces$Longitude <- ifelse(provinces$Province == "Sta Cruz De Tenerife",
                             -11.3, provinces$Longitude)

#Merge Provinces and communities
provinces_communities <- merge(provinces, autonomous_com, by = "Autonomous_community")

# Merge Retailers and Provinces
retailers_capitals <- merge(retailers, provinces_communities, by.x = "region", by.y = "Province")

# Merge Orders, Retailers and Provinces
orders_retailers_capitals <- merge(orders_prices, retailers_capitals, by = "id_establishment")

orders_retailers_capitals <- orders_retailers_capitals %>% distinct(
  id_establishment,material, date ,quantity, price, Capital, Latitude, Longitude,
  Autonomous_community ,Population_community,Population, price_liter, Beer_consumption)

#filter for year 2020
orders_retailers_capitals_year <- orders_retailers_capitals %>%
  filter(date >= "2020-01-01" & date <= "2020-12-31")
```

Aggregated and per capital field were added to represent sales money and volume wise for the different provinces of Spain.
```{r}
#summary for map
#adding revenue
orders_retailers_capitals_year <- orders_retailers_capitals_year %>%
  mutate(Revenue = price_liter * quantity)

Revenue_region <- orders_retailers_capitals_year %>% group_by(Capital) %>%
  summarise(Sales = sum(Revenue), Volume = sum(quantity),
            Population_province = mean(Population),
            Longitude = mean(Longitude), Latitude = mean(Latitude))

Revenue_region <- Revenue_region %>% mutate(sales_per_capita = Sales / Population_province)
Revenue_region <- Revenue_region %>% mutate(Volume_per_capita = Volume / Population_province)
```

### Sales by volume

After preparing the data for Spain, two plots were prepared to represent 
the sales of Damm for every province. The map on the left represent the amount of
liters sold in *2020*. Additionally, in order to consider the effect of the population,on the right it is possible to see the representation of the average consumption of beer Damm in the same year. 

```{r, fig.align='center', out.width='100%'}
Spain <- esp_get_prov()

map_vol_1 <- ggplot(Spain) + geom_sf(fill = "#4ED09A", color = "#000000") +
  geom_point(data = Revenue_region, aes(x = Longitude, y = Latitude, size = Volume),
             color = "#124C6F") +
  theme(axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10),
        legend.title =  element_text(size = 8),
        legend.text = element_text(size = 8),
        legend.position = "bottom",
    panel.background = element_rect(fill = "lightblue", colour = "lightblue")) +
  scale_size_continuous(name = "Total Volume")

map_vol_2 <- ggplot(Spain) + geom_sf(fill = "#4ED09A", color = "#000000") + 
  geom_point(data = Revenue_region, aes(x = Longitude, y = Latitude,
                                        size = Volume_per_capita),
             color = "#124C6F") +
  theme(axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10),
        legend.title =  element_text(size = 8),
        legend.text = element_text(size = 8),
        legend.position = "bottom",
    panel.background = element_rect(fill = "lightblue", colour = "lightblue")) +
  scale_size_continuous(name = "Volume per capita")

grid.arrange(map_vol_1, map_vol_2, ncol = 2)
```
Barcelona is by far the province in which the biggest volume of Damm products are sold,
with more than **24** millions of liters. In Tarragona, Madrid and Palma around half a million of liters are distributed.
Now, when comparing the amount sold with the population other facts can be noticed. In this case, Tarragona has the highest consumption per capita a year, with almost 7 liters. Barcelona, Gerona, Logroño and Palma also have remarkable consumption of Damm products with around 4 liters.

### Sales expressed in money

It is also convenient to check the performance of the company in the provinces with the results expressed in terms of money. The two maps presented below show the *total of sales* and the *sales per capita* for 2020.

```{r, fig.align='center', out.width='100%'}
map_sales_1 <- ggplot(Spain) + geom_sf(fill = "#4ED09A", color = "#000000") +
  geom_point(data = Revenue_region, aes(x = Longitude, y = Latitude, size = Sales),
             color = "#124C6F") +
  theme(axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10),
        legend.title =  element_text(size = 8),
        legend.text = element_text(size = 8),
        legend.position = "bottom",
    panel.background = element_rect(fill = "lightblue", colour = "lightblue")) +
  scale_size_continuous(name = "Total Sales")

map_sales_2 <- ggplot(Spain) + geom_sf(fill = "#4ED09A", color = "#000000") + 
  geom_point(data = Revenue_region, aes(x = Longitude, y = Latitude,
                                        size = sales_per_capita), color = "#124C6F") + 
  theme(axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10),
        legend.title =  element_text(size = 8),
        legend.text = element_text(size = 8),
        legend.position = "bottom",
    panel.background = element_rect(fill = "lightblue", colour = "lightblue")) +
  scale_size_continuous(name = "Sales per capita")

grid.arrange(map_sales_1, map_sales_2, ncol = 2)
```

The results are very similar to those of the sales expressed in volume with a couple of exceptions. Considering the total of sales, Gerona became more important than Palma in terms of money. Regarding the consumption per capita, Logroño has a lower performance, going from second to fourth position.

### Company vs market performance

In the previous section, the performance of Damm was analyzed in the provinces throughout Spain. However, it was considered interesting revising the behavior of sales in comparison with the whole market. To do this, the spending of people in beer was investigated in external sources **(Stata)**. This information was represented in the map below. The community with the highest consumption was Islas Baleares, with **€39.4** on average in *2020*, probably boosted by tourism. On the other hand, in Asturias people spent **€21.3** on average for the same period. In Catalonia, in turn, this number was **€33**.

```{r, fig.align='center'}
#Summary autonomous community
Revenue_Autonomous_community <- orders_retailers_capitals_year %>%
  group_by(Autonomous_community) %>% 
  summarise(Sales = sum(Revenue), Volume = sum(quantity),
            Population_community = mean(Population_community),
            Beer_consumption_per_capita = mean(Beer_consumption),
            Longitude = mean(Longitude), Latitude = mean(Latitude))

Revenue_Autonomous_community <- Revenue_Autonomous_community %>%
  mutate(Sales_per_capita = Sales / Population_community,
         Volume_per_capita = Volume / Population_community,
         Damm_market_percentage = Sales_per_capita/Beer_consumption_per_capita)

Spain_ccaa <- esp_get_ccaa()

ggplot(Spain_ccaa) + geom_sf(fill = "#4ED09A", color = "#000000") +
  geom_point(data = Revenue_Autonomous_community,
             aes(x = Longitude, y = Latitude,
                 size = Beer_consumption_per_capita),
                 color = "#004168") +
  theme(axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10),
        legend.title =  element_text(size = 8),
        legend.text = element_text(size = 8),
        legend.position = "right",
    panel.background = element_rect(fill = "lightblue", colour = "lightblue")) +
  scale_size_continuous(name = "Market spending per capita (year)")


```

Regarding the performance of the company, it has the best results in Catalonia with sales of **8.5** euros per person per year. In Rioja, the outcome was **€7** and in Balearic Island it was **5.7**. On the other side, in 8 of the 17 Autonomous Communities, the consumption per capita was less than **€1** per year, that is, the presence of the companies in those territories is minor.

```{r, fig.align='center'}
ggplot(Spain_ccaa) + geom_sf(fill = "#4ED09A", color = "#000000") +
  geom_point(data = Revenue_Autonomous_community,
             aes(x = Longitude, y = Latitude, size = Sales_per_capita),
             color = "#004168") +
    theme(axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10),
        legend.title =  element_text(size = 8),
        legend.text = element_text(size = 8),
        legend.position = "right",
    panel.background = element_rect(fill = "lightblue", colour = "lightblue")) +
  scale_size_continuous(name = "Damm sales per capita (year)")
```

To verify the previous analysis, the ratio of the per capita consumption of Damm and that of the whole market was prepared. The results are presented in the following map.

```{r, fig.align='center'}
ggplot(Spain_ccaa) + geom_sf(fill = "#4ED09A", color = "#000000") +
  geom_point(data = Revenue_Autonomous_community,
             aes(x = Longitude, y = Latitude, size = Damm_market_percentage),
                 color = "#004168") +
  theme(axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10),
        legend.title =  element_text(size = 8),
        legend.text = element_text(size = 8),
        legend.position = "right",
    panel.background = element_rect(fill = "lightblue", colour = "lightblue")) +
  scale_size_continuous(name = "Ratio Damm - market")
```

According to the calculations, in Catalonia and Rioja **25** cents out of every euro people spend on beer, goes to the Damm unit of business under consideration. Nevertheless, in 11 of the Autonomous Communities, this number is below **5** cents.

In this sense, the company has a great potential of growth in the Communities where so far is not a dominant player.


# RFM

Given the historical data about retailers and transactions, it is possible to perform and analysis of Recency, Frequency and Money [spent]. To do this, a revenue column was added.

The *rfm* library was used. The classification is done with the function *rfm_table_order*. In the sake of the analysis, the final date was set at the end of 2020.

```{r, fig.align='center'}
#add column revenue
orders_prices <- orders_prices%>% mutate(revenue = quantity * price_liter)
orders_prices_rfm <- orders_prices %>% filter(date <= "2020-12-31")

### ----create rfm score -------------------------------------------------------
rfm_result <- rfm_table_order(orders_prices_rfm, customer, date, revenue, as.Date("2020-12-31"))
```

## Visual analysis of RFM

The following plots are helpful representing the general distribution of the clients, according to the variables *frequency, recency and money*. For instance, the heat map shown below is useful by telling that most of the revenue is given by the customers which more frequently and most recently have purchased products of the company.

```{r, fig.align='center'}
## ----heat map of the situation ----------------------------------------------
rfm_heatmap(rfm_result)
```

With the plot below it is possible to assert that the clients with highest recency and frequency, tend to have a high monetary score. That is to say, in the recent history, the most frequent clients, spend important amounts of money, which is good for the company.
```{r, fig.align='center'}
## ----bar chart of the situation ----------------------------------------------
rfm_bar_chart(rfm_result)
```
When the whole data is considered, it is possible to see that the distribution of clients in the three variables tend to be asymmetrical, in which most of them do orders of low money, have ordered very recently and have low frequency. 
This implies that a small bunch of customers have spent big amounts of money in comparison with the rest.
```{r, fig.align='center'}
## ----histogram---------------------------------------------------------------
rfm_histograms(rfm_result)
```
This can be validated by the scatter plot below. There are 5 client that outperform the vast majority. It is observed a trend that says the more frequency, the more money spent.

```{r, fig.align='center'}
## ----histogram---------------------------------------------------------------
rfm_fm_plot(rfm_result)
```
Not surprisingly, clients with better recency score, tend to have higher expenditure and frequency. There are a couple of outliers in the relation recency-frequency. Two clients that used to have a high frequency for a relatively low recency. Those could be customers to take special care of.
```{r, fig.align='center'}
## ----some scatter plots---------------------------------------------------------
rfm_rm_plot(rfm_result)

rfm_rf_plot(rfm_result)
```

## RFM clients segmentation

After the preliminary analysis, in the paragraphs below a segmentation is presented according to the RFM scores. First, 11 categories were defined. For example, if certain client has a high value of Recency, Frequency and Money, it is considered a *champion*. If, on the contrary, it has very low values for all the categories, it is a *lost* client. This classification is useful in defining strategies for each client.
```{r, fig.align='center'}
## ======RFM - segments=========================================================

segment_names <- c("Champions", "Loyal Customers", "Potential Loyalist",
                   "New Customers", "Promising", "Need Attention", "About To Sleep",
                   "At Risk", "Can't Lose Them", "Hibernating", "Lost")

recency_lower   <- c(4, 2, 3, 4, 3, 3, 2, 1, 1, 2, 1)
recency_upper   <- c(5, 4, 5, 5, 4, 4, 3, 2, 1, 3, 1)
frequency_lower <- c(4, 3, 1, 1, 1, 3, 1, 2, 4, 2, 1)
frequency_upper <- c(5, 4, 3, 1, 1, 4, 2, 5, 5, 3, 1)
monetary_lower  <- c(4, 4, 1, 1, 1, 3, 1, 2, 4, 2, 1)
monetary_upper  <- c(5, 5, 3, 1, 1, 4, 2, 5, 5, 3, 1)

segments <- rfm_segment(rfm_result, segment_names, recency_lower, recency_upper,
                        frequency_lower, frequency_upper, monetary_lower, monetary_upper)

```

The table below presents how many client Damm have in each category. Additionally, the median of the relevant values: Number of transactions (frequency), Recency days (Recency) and Expenditure (Money).
```{r, fig.align='center'}
segments %>% group_by(segment) %>%
  summarise(Count = n(), Transactions_med = median(transaction_count),
            Recency_day_med = median(recency_days), Expenditure_med = median(amount))

```
More details about the content of this table, are shown in the following graphs.

For example, the plot below allows to analyze the distribution of money for the proposed categories. For example, clients of the type *Champion* have purchased with a median value of roughly 3.5 million euros. 
```{r, fig.align='center'}
## ----average monetary value----------------------------------------------
rfm_plot_median_monetary(segments)
```

Regarding recency, clients with more than 1000 days with transactions can be considered lost. Conversely, the *champions* are around 30 days.
```{r, fig.align='center'}
## ----average recency-----------------------------------------------------
rfm_plot_median_recency(segments)
```

Also, when analyzing *frequency*, it is possible to observe that for *lost* and *about to sleep* clients, they just register a few orders. On the other side, *champions* typically have several hundreds.
```{r, fig.align='center'}
## ----average frequency---------------------------------------------------
rfm_plot_median_frequency(segments)
```

There are several segments which deserve special attention. For instance, *Need attention* or *Others*, which had median expenditures of roughly half a million euros, but with recency days over 100. These are important types of clients that might be in danger. It is required to take care of them.

Additionally, the category *Potential Loyalist* has 67 customers, which had purchased 0.18 million as median, which is not negligible. However, the frequency is low, but they are recent. These are new client that need attention to consolidate the commercial relation.

#CLV

```{r}
orders_prices_clv <- filter(orders_prices, date >= '2018-01-01' & date <= '2021-06-01')

orders_prices_clv <- orders_prices_clv %>%
  group_by(customer) %>%
  filter(!all(date > as.Date('2021-01-01'))) %>%
  ungroup -> result

clv.apparel <- clvdata(orders_prices_clv,
  date.format = "ymd",
  time.unit = "week",
  estimation.split = 156,
  name.id = "customer",
  name.date = "date",
  name.price = "revenue"
)

clv.apparel
summary (clv.apparel)
```

### The Estimation Pareto 

```{r}
est.pnbd <- pnbd(clv.data = clv.apparel)
est.pnbd
plot(est.pnbd)
```

### The prediction for the holdout period
```{r}
pred_holdout <- predict(est.pnbd, predict.spending=TRUE)
print(pred_holdout)
print(pred_holdout)[1] 

plot(est.pnbd, cumulative = FALSE)
plot(est.pnbd, cumulative = TRUE)
```


# Association rule analysis

The following code prepares the transaction matrix by creating a fictional transaction ID to link products that were bought on the same day by the same client. After this is done the matrix is created to use to mine all the relevant association rules.

```{r}
##################################
### Prepare transaction matrix ###
##################################
date_customer <- data.frame(orders_prices$date,orders_prices$customer)
colnames(date_customer) <- c("date","customer")

date_customer <- date_customer %>%
  group_by(customer, date) %>%
  unique()
# Create fictional transaction ID to mine asociation rules
test <- date_customer %>% rowid_to_column(var='transaction_id')
new_data <- merge(orders_prices,test)

# Create matrix
table <- new_data %>%
  select(transaction_id, type) %>%
  distinct() %>%
  mutate(value = TRUE) %>%
  pivot_wider(transaction_id, names_from = type, values_from = value)

matrix <- as(table %>% select(-transaction_id), "transactions")
```

Mining the association rules with a support of at least 0.01, and a confidence of at least 0.8. There is slightly more than 10.000 transactions, this means that for the association rules mined the item combination occurs in at least around 100 transactions. And a confidence level of 0.8 means that in 80% of the cases this lead to the item on the right hand. With these confidence and suport settings 359 association rules are mined. On the right hand side the most dominant product categories are Daura, Turia, Lemon, Free, and Estrella, which will be dived into deeper in the following sections.

```{r}
########################################
### Mine and prune association rules ###
########################################
rules <- apriori(matrix,
                 control = list(verbose = FALSE),
                 parameter = list(supp = 0.01, conf = 0.8)
                 )

quality(rules) <- round(quality(rules), digits=3) 
rules <- sort(rules, by="lift")

subset_matrix <- is.subset(rules,rules)
subset_matrix[lower.tri(subset_matrix, diag = T)] <- FALSE
redundant <- colSums(subset_matrix, na.rm = T) >= 1
rules_pruned <- rules[!redundant]
rules_df <- as(rules_pruned, "data.frame")[ , c(1:3, 5)]
```

The following table shows the top 10 association rules based on lift. 

```{r}
top_rules <- rules_df %>% head(10)
formattable(top_rules, 
            align =c("c","c","c","c"), 
            list(`Indicator Name` = formatter(
              "span", style = ~ style(color = "grey",font.weight = "bold"))))
```

The following scatter plot shows the mined association rules with on the X-Axis the support, on the Y-Axis the confidence, and in the color grading the lift.

```{r}
plot(rules_pruned, measure = c("support", "confidence"))
```

## Daura product group relevant association rules

Since Daura is part of the more dominant product categories present in the previously mined association rules, the following code will mine rules specific for this product category.

```{r}
##################################################
### Mine and prune association rules for Daura ###
##################################################
rules <- apriori(matrix,
                 control = list(verbose = FALSE),
                 parameter = list(supp = 0.01, conf = 0.8),
                 appearance = list(rhs = "DAURA")
                 )

quality(rules) <- round(quality(rules), digits=3) 
rules <- sort(rules, by="lift")

subset_matrix <- is.subset(rules,rules)
subset_matrix[lower.tri(subset_matrix, diag = T)] <- FALSE
redundant <- colSums(subset_matrix, na.rm = T) >= 1
rules_pruned <- rules[!redundant]
rules_df <- as(rules_pruned, "data.frame")[ , c(1:3, 5)]
```

The following table shows the top relevant association rules for the Daura product group. This means that any customer that buys the products in the left hand side is very likely to buy a Daura product.

```{r}
top_rules <- rules_df %>% head(10)
formattable(top_rules, 
            align =c("c","c","c","c"), 
            list(`Indicator Name` = formatter(
              "span", style = ~ style(color = "grey",font.weight = "bold"))))
```

## Turia product group relevant association rules

Since Turia is part of the more dominant product categories present in the previously mined association rules, the following code will mine rules specific for this product category.

```{r}
##################################################
### Mine and prune association rules for Turia ###
##################################################
rules <- apriori(matrix,
                 control = list(verbose = FALSE),
                 parameter = list(supp = 0.01, conf = 0.8),
                 appearance = list(rhs = "TURIA")
                 )

quality(rules) <- round(quality(rules), digits=3) 
rules <- sort(rules, by="lift")

subset_matrix <- is.subset(rules,rules)
subset_matrix[lower.tri(subset_matrix, diag = T)] <- FALSE
redundant <- colSums(subset_matrix, na.rm = T) >= 1
rules_pruned <- rules[!redundant]
rules_df <- as(rules_pruned, "data.frame")[ , c(1:3, 5)]
```

The following table shows the top relevant association rules for the Turia product group. This means that any customer that buys the products in the left hand side is very likely to buy a Turia product.

```{r}
top_rules <- rules_df %>% head(10)
formattable(top_rules, 
            align =c("c","c","c","c"), 
            list(`Indicator Name` = formatter(
              "span", style = ~ style(color = "grey",font.weight = "bold"))))
```

## Lemon product group relevant association rules

Since Lemon is part of the more dominant product categories present in the previously mined association rules, the following code will mine rules specific for this product category.

```{r}
##################################################
### Mine and prune association rules for Lemon ###
##################################################
rules <- apriori(matrix,
                 control = list(verbose = FALSE),
                 parameter = list(supp = 0.01, conf = 0.8),
                 appearance = list(rhs = "LEMON")
                 )

quality(rules) <- round(quality(rules), digits=3) 
rules <- sort(rules, by="lift")

subset_matrix <- is.subset(rules,rules)
subset_matrix[lower.tri(subset_matrix, diag = T)] <- FALSE
redundant <- colSums(subset_matrix, na.rm = T) >= 1
rules_pruned <- rules[!redundant]
rules_df <- as(rules_pruned, "data.frame")[ , c(1:3, 5)]
```

The following table shows the top relevant association rules for the Lemon product group. This means that any customer that buys the products in the left hand side is very likely to buy a Lemon product.

```{r}
top_rules <- rules_df %>% head(10)
formattable(top_rules, 
            align =c("c","c","c","c"), 
            list(`Indicator Name` = formatter(
              "span", style = ~ style(color = "grey",font.weight = "bold"))))
```

## Free product group relevant association rules

Since Free is part of the more dominant product categories present in the previously mined association rules, the following code will mine rules specific for this product category.

```{r}
##################################################
### Mine and prune association rules for Free ####
##################################################
rules <- apriori(matrix,
                 control = list(verbose = FALSE),
                 parameter = list(supp = 0.01, conf = 0.8),
                 appearance = list(rhs = "FREE")
                 )

quality(rules) <- round(quality(rules), digits=3) 
rules <- sort(rules, by="lift")

subset_matrix <- is.subset(rules,rules)
subset_matrix[lower.tri(subset_matrix, diag = T)] <- FALSE
redundant <- colSums(subset_matrix, na.rm = T) >= 1
rules_pruned <- rules[!redundant]
rules_df <- as(rules_pruned, "data.frame")[ , c(1:3, 5)]
```

The following table shows the top relevant association rules for the Free product group. This means that any customer that buys the products in the left hand side is very likely to buy a Free product.

```{r}
top_rules <- rules_df %>% head(10)
formattable(top_rules, 
            align =c("c","c","c","c"), 
            list(`Indicator Name` = formatter(
              "span", style = ~ style(color = "grey",font.weight = "bold"))))
```

## Estrella product group relevant association rules

Since Free is part of the more dominant product categories present in the previously mined association rules, the following code will mine rules specific for this product category.

```{r}
######################################################
### Mine and prune association rules for Estrella ####
######################################################
rules <- apriori(matrix,
                 control = list(verbose = FALSE),
                 parameter = list(supp = 0.01, conf = 0.8),
                 appearance = list(rhs = "ESTRELLA")
                 )

quality(rules) <- round(quality(rules), digits=3) 
rules <- sort(rules, by="lift")

subset_matrix <- is.subset(rules,rules)
subset_matrix[lower.tri(subset_matrix, diag = T)] <- FALSE
redundant <- colSums(subset_matrix, na.rm = T) >= 1
rules_pruned <- rules[!redundant]
rules_df <- as(rules_pruned, "data.frame")[ , c(1:3, 5)]
```

The following table shows the top relevant association rules for the Estrella product group. This means that any customer that buys the products in the left hand side is very likely to buy a Estrella product.

```{r}
top_rules <- rules_df %>% head(10)
formattable(top_rules, 
            align =c("c","c","c","c"), 
            list(`Indicator Name` = formatter(
              "span", style = ~ style(color = "grey",font.weight = "bold"))))
```

# Conclusions 

+



# References 

* [1] R Core Team (2022). R: A language and environment for statistical computing. R
  Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
* [2]  Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software,
  4(43), 1686, https://doi.org/10.21105/joss.01686
* [3] Baptiste Auguie (2017). gridExtra: Miscellaneous Functions for "Grid" Graphics. R
  package version 2.3. https://CRAN.R-project.org/package=gridExtra
* [4] Sam Firke (2021). janitor: Simple Tools for Examining and Cleaning Dirty Data. R
  package version 2.1.0. https://CRAN.R-project.org/package=janitor
* [5] Garrett Grolemund, Hadley Wickham (2011). Dates and Times Made Easy with lubridate.
  Journal of Statistical Software, 40(3), 1-25. URL https://www.jstatsoft.org/v40/i03/.
* [6] Michael Hahsler, Christian Buchta, Bettina Gruen and Kurt Hornik (2022). arules: Mining
  Association Rules and Frequent Itemsets. R package version 1.7-3.
  https://CRAN.R-project.org/package=arules
* [7] Kun Ren and Kenton Russell (2021). formattable: Create 'Formattable' Data Structures. R
  package version 0.2.1. https://CRAN.R-project.org/package=formattable
* [8] Aravind Hebbali (2020). rfm: Recency, Frequency and Monetary Value Analysis. R package
  version 0.2.2. https://CRAN.R-project.org/package=rfm
* [9] Patrick Bachmann, Niels Kuebler, Markus Meierer, Jeffrey Naef, Elliot Oblander and
  Patrik Schilter (2022). CLVTools: Tools for Customer Lifetime Value Estimation. R
  package version 0.9.0. https://CRAN.R-project.org/package=CLVTools
* [10] Hernangómez D (2022). _mapSpain: Administrative Boundaries of Spain_. doi:
10.5281/zenodo.5366622 (URL: https://doi.org/10.5281/zenodo.5366622), <URL:
https://ropenspain.github.io/mapSpain/>.
* [11] Michael Hahsler (2021). arulesViz: Visualizing Association Rules and Frequent Itemsets.
  R package version 1.5-1. https://CRAN.R-project.org/package=arulesViz



